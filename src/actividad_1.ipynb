{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e5fdcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/python/3.12.1/lib/python3.12/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/codespace/.local/lib/python3.12/site-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in /home/codespace/.local/lib/python3.12/site-packages (from gdown) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4->gdown) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests[socks]->gdown) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests[socks]->gdown) (2025.7.9)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sqlite3 as sql\n",
    "import os\n",
    "\n",
    "# Instalar gdown para acceder al archivo en Drive\\n\"\n",
    "%pip install gdown\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804b5956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1YEdQA-dDrK9o6_H7sIy3LqbDk-RnWf0d\n",
      "To: /workspaces/Big_Data/src/dataset.csv\n",
      "100%|██████████| 12.4M/12.4M [00:00<00:00, 100MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo cargado exitosamente\n",
      "   OBJECTID                                     Shape radicado  \\\n",
      "0         1         (4713547.5105, 2247612.977400001)  1580885   \n",
      "1         2         (4715357.8105, 2243807.580100002)  1585081   \n",
      "2         3         (4710262.4967, 2249847.019700002)  1581508   \n",
      "3         4  (4715512.4542000005, 2246920.8745000027)  1581862   \n",
      "4         5   (4713597.201000002, 2252326.8505000006)  1578199   \n",
      "\n",
      "                 fecha      hora  dia  periodo           clase  \\\n",
      "0  2017-05-05 00:00:00  02:00 PM    5     2017          Choque   \n",
      "1  2017-06-06 00:00:00  11:20 AM    6     2017          Choque   \n",
      "2  2017-05-10 00:00:00  12:00 PM   10     2017          Choque   \n",
      "3  2017-05-12 00:00:00  05:30 PM   12     2017  Caida Ocupante   \n",
      "4  2017-04-14 00:00:00  04:30 AM   14     2017          Choque   \n",
      "\n",
      "           direccion                direccion_enc  ...           barrio  \\\n",
      "0  CL 32 Norte CR 69     CL  032   069  000 00000  ...          Rosales   \n",
      "1   CL 4 Sur CR 43 B  CL S 004   043 B  000 00000  ...     Patio Bonito   \n",
      "2       CL 40 CR 105     CL  040   105  000 00000  ...  San Javier No.1   \n",
      "3        CL 28 CR 44     CL  028   044  000 00000  ...  Barrio Colombia   \n",
      "4        CL 76 CR 80     CL  076   080  000 00000  ...      Villa Flora   \n",
      "\n",
      "       comuna        diseno dia_nombre mes mes_nombre   longitud   latitud  \\\n",
      "0       Belén  Tramo de via  VIERNES     5        NaN -75.589659  6.234581   \n",
      "1  El Poblado  Tramo de via  MARTES      6        NaN -75.573137  6.200258   \n",
      "2  San Javier  Tramo de via  MIÉRCOLES   5        NaN -75.619437  6.254631   \n",
      "3  El Poblado  Tramo de via  VIERNES     5        NaN -75.571877  6.228411   \n",
      "4     Robledo  Tramo de via  VIERNES     4        NaN -75.589420  6.277200   \n",
      "\n",
      "   x_origen_nacional  y_origen_nacional  \n",
      "0       4.713548e+06       2.247613e+06  \n",
      "1       4.715358e+06       2.243808e+06  \n",
      "2       4.710262e+06       2.249847e+06  \n",
      "3       4.715512e+06       2.246921e+06  \n",
      "4       4.713597e+06       2.252327e+06  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Forma del dataset: (46210, 23)\n"
     ]
    }
   ],
   "source": [
    "# ID del archivo de Google Drive\n",
    "file_id = \"1YEdQA-dDrK9o6_H7sIy3LqbDk-RnWf0d\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Descargar el archivo\n",
    "output = \"dataset.csv\"\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "# Leer el archivo\n",
    "try:\n",
    "    df = pd.read_csv(output, encoding=\"ISO-8859-1\")\n",
    "    print(\"✅ Archivo cargado exitosamente\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nForma del dataset: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3cdf3",
   "metadata": {},
   "source": [
    "# Problemática y dataset\n",
    "\n",
    "1. Problema:  Identificar patrones y tendencias en los incidentes de tránsito, incluyendo su distribución geográfica y temporal.\n",
    "2. Variables relevantes: \n",
    "\n",
    "*Categóricos: clase, dia_nombre, barrio, comuna, diseno.\n",
    "*Numéricos: Coordenadas (longitud, latitud).\n",
    "*Fecha y hora: fecha, hora.\n",
    "\n",
    "3. Enlace: [Dataset](https://www.medellin.gov.co/geomedellin/datosAbiertos/275)\n",
    "\n",
    "Se eligio este dataset con el fin de identificar puntos criticos de mayor indice de accidentabilidad en la ciudad de medellín, el fin es poder generar alertas tempranas en dichos puntos y realizar campañas de sensibilización a los ciudadanos y también implementar medidas como el uso de resaltos, implementación de señalizaciones y también instalación de semaforización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520cc11d",
   "metadata": {},
   "source": [
    "# Modelo entidad–relación (ERD)\n",
    "\n",
    "![Modelo_ER](docs/Modelo_ER.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a8f07",
   "metadata": {},
   "source": [
    "El diagrama representa un sistema que organiza los incidentes de tránsito en tres niveles:\n",
    "\n",
    "\n",
    "1. Comuna: Es la unidad territorial más amplia. Cada comuna agrupa varios barrios.\n",
    "\n",
    "2. Barrio: Cada barrio pertenece a una sola comuna y actúa como punto intermedio entre la comuna y los incidentes.\n",
    "\n",
    "3. Incidente: Contiene toda la información detallada de cada evento de tránsito: fecha, hora, gravedad, tipo de incidente, ubicación y coordenadas. Cada incidente ocurre en un único barrio.\n",
    "\n",
    "\n",
    "En conjunto, el modelo muestra que:\n",
    "\n",
    "°Una comuna puede tener muchos barrios.\n",
    "\n",
    "°Un barrio puede tener muchos incidentes.\n",
    "\n",
    "°Cada incidente queda conectado geográfica y administrativamente mediante su barrio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5658c64",
   "metadata": {},
   "source": [
    "# 3️⃣ Crea la base de datos e inserta información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f63b6605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el CSV con pandas\n",
    "csv_file = 'dataset.csv'\n",
    "if not os.path.exists(csv_file):\n",
    "    raise FileNotFoundError(f\"El archivo {csv_file} no se encuentra en el directorio actual.\")\n",
    "df = pd.read_csv(csv_file, sep=',', encoding='latin1')  # Ajusta el separador si es necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ac993ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conectar a SQLite (crea la base de datos si no existe)\n",
    "conn = sql.connect('incidentes_transito.bd')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14e3d004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x747bee2dfbc0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear tablas según el Modelo ER\n",
    "# Habilitar claves foráneas\n",
    "cursor.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "# Tabla Ubicación\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS Ubicacion (\n",
    "    cbml TEXT PRIMARY KEY,\n",
    "    direccion TEXT,\n",
    "    direccion_enc TEXT,\n",
    "    barrio TEXT,\n",
    "    comuna TEXT,\n",
    "    longitud REAL,\n",
    "    latitud REAL,\n",
    "    x_origen_nacional REAL,\n",
    "    y_origen_nacional REAL,\n",
    "    shape TEXT\n",
    ");\n",
    "''')\n",
    "# Tabla Tiempo\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS Tiempo (\n",
    "    fecha TEXT,\n",
    "    hora TEXT,\n",
    "    dia INTEGER,\n",
    "    periodo INTEGER,\n",
    "    dia_nombre TEXT,\n",
    "    mes INTEGER,\n",
    "    mes_nombre TEXT,\n",
    "    PRIMARY KEY (fecha, hora)\n",
    ");\n",
    "''')\n",
    "# Tabla Tipo_Incidente\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS Tipo_Incidente (\n",
    "    clase TEXT PRIMARY KEY,\n",
    "    gravedad TEXT\n",
    ");\n",
    "''')\n",
    "# Tabla Incidente (con claves foráneas)\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS Incidente (\n",
    "    radicado TEXT PRIMARY KEY,\n",
    "    clase TEXT,\n",
    "    gravedad TEXT,\n",
    "    diseno TEXT,\n",
    "    tipo_geocod TEXT,\n",
    "    objectid INTEGER,\n",
    "    cbml TEXT,\n",
    "    fecha TEXT,\n",
    "    hora TEXT,\n",
    "    FOREIGN KEY (cbml) REFERENCES Ubicacion(cbml),\n",
    "    FOREIGN KEY (fecha, hora) REFERENCES Tiempo(fecha, hora),\n",
    "    FOREIGN KEY (clase) REFERENCES Tipo_Incidente(clase)\n",
    ");\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a0e8943",
   "metadata": {},
   "outputs": [
    {
     "ename": "IntegrityError",
     "evalue": "UNIQUE constraint failed: Ubicacion.cbml",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIntegrityError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Insertar datos en las tablas (normalizando y evitando duplicados en claves primarias)\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Insertar en Ubicacion (eliminar duplicados basados en cbml)\u001b[39;00m\n\u001b[32m      4\u001b[39m ubicacion_data = df[[\u001b[33m'\u001b[39m\u001b[33mcbml\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdireccion\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdireccion_enc\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbarrio\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcomuna\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlongitud\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlatitud\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mx_origen_nacional\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33my_origen_nacional\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mShape\u001b[39m\u001b[33m'\u001b[39m]].drop_duplicates(subset=[\u001b[33m'\u001b[39m\u001b[33mcbml\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mubicacion_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mUbicacion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mappend\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Insertar en Tiempo (eliminar duplicados basados en fecha y hora)\u001b[39;00m\n\u001b[32m      7\u001b[39m tiempo_data = df[[\u001b[33m'\u001b[39m\u001b[33mfecha\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhora\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdia\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mperiodo\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdia_nombre\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmes\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmes_nombre\u001b[39m\u001b[33m'\u001b[39m]].drop_duplicates(subset=[\u001b[33m'\u001b[39m\u001b[33mfecha\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhora\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/generic.py:3106\u001b[39m, in \u001b[36mNDFrame.to_sql\u001b[39m\u001b[34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[39m\n\u001b[32m   2908\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2909\u001b[39m \u001b[33;03mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[32m   2910\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3102\u001b[39m \u001b[33;03m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[32m   3103\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m   3104\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sql\n\u001b[32m-> \u001b[39m\u001b[32m3106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3117\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/sql.py:844\u001b[39m, in \u001b[36mto_sql\u001b[39m\u001b[34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    840\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mframe\u001b[39m\u001b[33m'\u001b[39m\u001b[33m argument should be either a Series or a DataFrame\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    841\u001b[39m     )\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con, schema=schema, need_transaction=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/sql.py:2841\u001b[39m, in \u001b[36mSQLiteDatabase.to_sql\u001b[39m\u001b[34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[39m\n\u001b[32m   2831\u001b[39m table = SQLiteTable(\n\u001b[32m   2832\u001b[39m     name,\n\u001b[32m   2833\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2838\u001b[39m     dtype=dtype,\n\u001b[32m   2839\u001b[39m )\n\u001b[32m   2840\u001b[39m table.create()\n\u001b[32m-> \u001b[39m\u001b[32m2841\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/sql.py:1121\u001b[39m, in \u001b[36mSQLTable.insert\u001b[39m\u001b[34m(self, chunksize, method)\u001b[39m\n\u001b[32m   1118\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1120\u001b[39m chunk_iter = \u001b[38;5;28mzip\u001b[39m(*(arr[start_i:end_i] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m data_list))\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m num_inserted = \u001b[43mexec_insert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[38;5;66;03m# GH 46891\u001b[39;00m\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_inserted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/sql.py:2537\u001b[39m, in \u001b[36mSQLiteTable._execute_insert\u001b[39m\u001b[34m(self, conn, keys, data_iter)\u001b[39m\n\u001b[32m   2535\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_execute_insert\u001b[39m(\u001b[38;5;28mself\u001b[39m, conn, keys, data_iter) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   2536\u001b[39m     data_list = \u001b[38;5;28mlist\u001b[39m(data_iter)\n\u001b[32m-> \u001b[39m\u001b[32m2537\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutemany\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minsert_statement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m conn.rowcount\n",
      "\u001b[31mIntegrityError\u001b[39m: UNIQUE constraint failed: Ubicacion.cbml"
     ]
    }
   ],
   "source": [
    "# Insertar datos en las tablas (normalizando y evitando duplicados en claves primarias)\n",
    "\n",
    "# Insertar en Ubicacion (eliminar duplicados basados en cbml)\n",
    "ubicacion_data = df[['cbml', 'direccion', 'direccion_enc', 'barrio', 'comuna', 'longitud', 'latitud', 'x_origen_nacional', 'y_origen_nacional', 'Shape']].drop_duplicates(subset=['cbml'])\n",
    "ubicacion_data.to_sql('Ubicacion', conn, if_exists='append', index=False)\n",
    "# Insertar en Tiempo (eliminar duplicados basados en fecha y hora)\n",
    "tiempo_data = df[['fecha', 'hora', 'dia', 'periodo', 'dia_nombre', 'mes', 'mes_nombre']].drop_duplicates(subset=['fecha', 'hora'])\n",
    "tiempo_data.to_sql('Tiempo', conn, if_exists='append', index=False)\n",
    "# Insertar en Tipo_Incidente (eliminar duplicados basados en clase)\n",
    "tipo_incidente_data = df[['clase', 'gravedad']].drop_duplicates(subset=['clase'])\n",
    "tipo_incidente_data.to_sql('Tipo_Incidente', conn, if_exists='append', index=False)\n",
    "# Insertar en Incidente (eliminar duplicados basados en radicado)\n",
    "incidente_data = df[['radicado', 'clase', 'gravedad', 'diseno', 'tipo_geocod', 'OBJECTID', 'cbml', 'fecha', 'hora']].drop_duplicates(subset=['radicado'])\n",
    "incidente_data.to_sql('Incidente', conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8b90969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo ER creado exitosamente en 'incidentes_transito.db'.\n",
      "Puedes consultar las tablas con consultas SQL estándar.\n"
     ]
    }
   ],
   "source": [
    "print(\"Modelo ER creado exitosamente en 'incidentes_transito.db'.\")\n",
    "print(\"Puedes consultar las tablas con consultas SQL estándar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aa4cd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla 'Ubicacion': 404 filas cargadas.\n",
      "Tabla 'Tiempo': 30469 filas cargadas.\n",
      "Tabla 'Tipo_Incidente': 9 filas cargadas.\n",
      "Tabla 'Incidente': 46204 filas cargadas.\n"
     ]
    }
   ],
   "source": [
    "tablas = ['Ubicacion', 'Tiempo', 'Tipo_Incidente', 'Incidente']\n",
    "for tabla in tablas:\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {tabla};\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"Tabla '{tabla}': {count} filas cargadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbb6c3a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "Cannot operate on a closed database.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mProgrammingError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(df_result)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tabla \u001b[38;5;129;01min\u001b[39;00m tablas:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[43mmostrar_evidencias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtabla\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Cerrar conexión\u001b[39;00m\n\u001b[32m     10\u001b[39m conn.close()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mmostrar_evidencias\u001b[39m\u001b[34m(tabla)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmostrar_evidencias\u001b[39m(tabla):\n\u001b[32m      3\u001b[39m     query = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSELECT * FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtabla\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m LIMIT 5;\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     df_result = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Evidencias de carga en tabla \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtabla\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (primeras 5 filas) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(df_result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/sql.py:528\u001b[39m, in \u001b[36mread_sql_query\u001b[39m\u001b[34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/sql.py:2728\u001b[39m, in \u001b[36mSQLiteDatabase.read_query\u001b[39m\u001b[34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m   2717\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_query\u001b[39m(\n\u001b[32m   2718\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2719\u001b[39m     sql,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2726\u001b[39m     dtype_backend: DtypeBackend | Literal[\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2727\u001b[39m ) -> DataFrame | Iterator[DataFrame]:\n\u001b[32m-> \u001b[39m\u001b[32m2728\u001b[39m     cursor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2729\u001b[39m     columns = [col_desc[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor.description]\n\u001b[32m   2731\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/sql.py:2662\u001b[39m, in \u001b[36mSQLiteDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   2660\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mQuery must be a string unless using sqlalchemy.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2661\u001b[39m args = [] \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [params]\n\u001b[32m-> \u001b[39m\u001b[32m2662\u001b[39m cur = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2663\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2664\u001b[39m     cur.execute(sql, *args)\n",
      "\u001b[31mProgrammingError\u001b[39m: Cannot operate on a closed database."
     ]
    }
   ],
   "source": [
    "# Mostrar evidencias actualizadas (SELECT * LIMIT 5 para todas las tablas)\n",
    "def mostrar_evidencias(tabla):\n",
    "    query = f\"SELECT * FROM {tabla} LIMIT 5;\"\n",
    "    df_result = pd.read_sql_query(query, conn)\n",
    "    print(f\"\\n--- Evidencias de carga en tabla '{tabla}' (primeras 5 filas) ---\")\n",
    "    print(df_result)\n",
    "for tabla in tablas:\n",
    "    mostrar_evidencias(tabla)\n",
    "# Cerrar conexión\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c13fc",
   "metadata": {},
   "source": [
    "# 4️⃣ Evidencia con consultas SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a287a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file = 'incidentes_transito.db'\n",
    "csv_file = 'total_incidentes_transito.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5f017d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV cargado con 46210 filas.\n",
      "Columnas del DataFrame: ['OBJECTID', 'Shape', 'radicado', 'fecha', 'hora', 'dia', 'periodo', 'clase', 'direccion', 'direccion_enc', 'cbml', 'tipo_geocod', 'gravedad', 'barrio', 'comuna', 'diseno', 'dia_nombre', 'mes', 'mes_nombre', 'longitud', 'latitud', 'x_origen_nacional', 'y_origen_nacional']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x747bee5e9840>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 1: Forzar eliminación y recreación de la DB\n",
    "db_file = 'incidentes_transito.db'\n",
    "csv_file = 'dataset.csv'\n",
    "if os.path.exists(db_file):\n",
    "    os.remove(db_file)\n",
    "    print(\"Base de datos existente eliminada. Recreando...\")\n",
    "# Leer CSV y verificar columnas\n",
    "if not os.path.exists(csv_file):\n",
    "    raise FileNotFoundError(f\"El archivo {csv_file} no se encuentra.\")\n",
    "df = pd.read_csv(csv_file, sep=',', encoding='latin1')\n",
    "print(f\"CSV cargado con {len(df)} filas.\")\n",
    "print(f\"Columnas del DataFrame: {list(df.columns)}\")  # Verificar nombres exactos\n",
    "# Conectar a nueva DB\n",
    "conn = sql.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"PRAGMA foreign_keys = ON;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "664ce22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla Ubicacion creada.\n",
      "Tabla Tiempo creada.\n",
      "Tabla Tipo_Incidente creada.\n",
      "Tabla Incidente creada.\n",
      "Ubicacion cargada con 404 filas.\n",
      "Tiempo cargada con 30469 filas.\n",
      "Tipo_Incidente cargada con 9 filas.\n",
      "Incidente cargada con 46204 filas.\n",
      "Base de datos creada y cargada exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Crear tablas con nombres de columnas EXACTOS del DataFrame (case-sensitive en pandas to_sql)\n",
    "\n",
    "try:\n",
    " cursor.execute('''\n",
    "    CREATE TABLE Ubicacion (\n",
    "        cbml TEXT PRIMARY KEY,\n",
    "        direccion TEXT,\n",
    "        direccion_enc TEXT,\n",
    "        barrio TEXT,\n",
    "        comuna TEXT,\n",
    "        longitud REAL,\n",
    "        latitud REAL,\n",
    "        x_origen_nacional REAL,\n",
    "        y_origen_nacional REAL,\n",
    "        Shape TEXT  -- Usar 'Shape' como en el DataFrame\n",
    "    );\n",
    "    ''')\n",
    " print(\"Tabla Ubicacion creada.\")\n",
    " cursor.execute('''\n",
    "    CREATE TABLE Tiempo (\n",
    "        fecha TEXT,\n",
    "        hora TEXT,\n",
    "        dia INTEGER,\n",
    "        periodo INTEGER,\n",
    "        dia_nombre TEXT,\n",
    "        mes INTEGER,\n",
    "        mes_nombre TEXT,\n",
    "        PRIMARY KEY (fecha, hora)\n",
    "    );\n",
    "    ''')\n",
    " print(\"Tabla Tiempo creada.\")\n",
    " cursor.execute('''\n",
    "    CREATE TABLE Tipo_Incidente (\n",
    "        clase TEXT PRIMARY KEY,\n",
    "        gravedad TEXT\n",
    "    );\n",
    "    ''')\n",
    " print(\"Tabla Tipo_Incidente creada.\")\n",
    "    \n",
    " cursor.execute('''\n",
    "    CREATE TABLE Incidente (\n",
    "        radicado TEXT PRIMARY KEY,\n",
    "        clase TEXT,\n",
    "        gravedad TEXT,\n",
    "        diseno TEXT,\n",
    "        tipo_geocod TEXT,\n",
    "        OBJECTID INTEGER,  -- Usar 'OBJECTID' como en el DataFrame\n",
    "        cbml TEXT,\n",
    "        fecha TEXT,\n",
    "        hora TEXT,\n",
    "        FOREIGN KEY (cbml) REFERENCES Ubicacion(cbml),\n",
    "        FOREIGN KEY (fecha, hora) REFERENCES Tiempo(fecha, hora),\n",
    "        FOREIGN KEY (clase) REFERENCES Tipo_Incidente(clase)\n",
    " );\n",
    "    ''')\n",
    " print(\"Tabla Incidente creada.\")\n",
    "\n",
    " # Cargar datos (asegurarse de que los nombres coincidan)\n",
    " ubicacion_data = df[['cbml', 'direccion', 'direccion_enc', 'barrio', 'comuna', 'longitud', 'latitud', 'x_origen_nacional', 'y_origen_nacional', 'Shape']].drop_duplicates(subset=['cbml'])\n",
    " ubicacion_data.to_sql('Ubicacion', conn, if_exists='append', index=False)\n",
    " print(f\"Ubicacion cargada con {len(ubicacion_data)} filas.\")\n",
    "    \n",
    " tiempo_data = df[['fecha', 'hora', 'dia', 'periodo', 'dia_nombre', 'mes', 'mes_nombre']].drop_duplicates(subset=['fecha', 'hora'])\n",
    " tiempo_data.to_sql('Tiempo', conn, if_exists='append', index=False)\n",
    " print(f\"Tiempo cargada con {len(tiempo_data)} filas.\")\n",
    "    \n",
    " tipo_incidente_data = df[['clase', 'gravedad']].drop_duplicates(subset=['clase'])\n",
    " tipo_incidente_data.to_sql('Tipo_Incidente', conn, if_exists='append', index=False)\n",
    " print(f\"Tipo_Incidente cargada con {len(tipo_incidente_data)} filas.\")\n",
    "    \n",
    " incidente_data = df[['radicado', 'clase', 'gravedad', 'diseno', 'tipo_geocod', 'OBJECTID', 'cbml', 'fecha', 'hora']].drop_duplicates(subset=['radicado'])\n",
    " incidente_data.to_sql('Incidente', conn, if_exists='append', index=False)\n",
    " print(f\"Incidente cargada con {len(incidente_data)} filas.\")\n",
    "    \n",
    " conn.commit()\n",
    " print(\"Base de datos creada y cargada exitosamente.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante creación o carga: {e}\")\n",
    "    conn.rollback()\n",
    "    conn.close()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e6d942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tablas existentes en la DB: ['Ubicacion', 'Tiempo', 'Tipo_Incidente', 'Incidente']\n"
     ]
    }
   ],
   "source": [
    "#Verificar tablas existentes\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tablas_existentes = cursor.fetchall()\n",
    "print(f\"Tablas existentes en la DB: {[t[0] for t in tablas_existentes]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65184316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutar consultas solicitadas\n",
    "tablas = ['Ubicacion', 'Tiempo', 'Tipo_Incidente', 'Incidente']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf5f102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Conteo de Registros ===\n",
      "Tabla 'Ubicacion': 404 registros.\n",
      "Interpretación: La tabla 'Ubicacion' contiene 404 filas, indicando el número total de entradas únicas. Un conteo bajo o cero sugiere problemas en la carga o datos faltantes.\n",
      "\n",
      "Tabla 'Tiempo': 30469 registros.\n",
      "Interpretación: La tabla 'Tiempo' contiene 30469 filas, indicando el número total de entradas únicas. Un conteo bajo o cero sugiere problemas en la carga o datos faltantes.\n",
      "\n",
      "Tabla 'Tipo_Incidente': 9 registros.\n",
      "Interpretación: La tabla 'Tipo_Incidente' contiene 9 filas, indicando el número total de entradas únicas. Un conteo bajo o cero sugiere problemas en la carga o datos faltantes.\n",
      "\n",
      "Tabla 'Incidente': 46204 registros.\n",
      "Interpretación: La tabla 'Incidente' contiene 46204 filas, indicando el número total de entradas únicas. Un conteo bajo o cero sugiere problemas en la carga o datos faltantes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conteo de registros\n",
    "print(\"\\n=== Conteo de Registros ===\")\n",
    "for tabla in tablas:\n",
    "    query = f\"SELECT COUNT(*) FROM {tabla};\"\n",
    "    result = pd.read_sql_query(query, conn)\n",
    "    count = result.iloc[0, 0]\n",
    "    print(f\"Tabla '{tabla}': {count} registros.\")\n",
    "    print(f\"Interpretación: La tabla '{tabla}' contiene {count} filas, indicando el número total de entradas únicas. Un conteo bajo o cero sugiere problemas en la carga o datos faltantes.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36049682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Nombres y Tipos de Columnas ===\n",
      "Tabla 'Ubicacion':\n",
      "                name  type\n",
      "0               cbml  TEXT\n",
      "1          direccion  TEXT\n",
      "2      direccion_enc  TEXT\n",
      "3             barrio  TEXT\n",
      "4             comuna  TEXT\n",
      "5           longitud  REAL\n",
      "6            latitud  REAL\n",
      "7  x_origen_nacional  REAL\n",
      "8  y_origen_nacional  REAL\n",
      "9              Shape  TEXT\n",
      "Interpretación: La tabla 'Ubicacion' tiene 10 columnas con tipos como TEXT (cadenas), INTEGER (enteros) y REAL (flotantes), esenciales para el Modelo ER.\n",
      "\n",
      "Tabla 'Tiempo':\n",
      "         name     type\n",
      "0       fecha     TEXT\n",
      "1        hora     TEXT\n",
      "2         dia  INTEGER\n",
      "3     periodo  INTEGER\n",
      "4  dia_nombre     TEXT\n",
      "5         mes  INTEGER\n",
      "6  mes_nombre     TEXT\n",
      "Interpretación: La tabla 'Tiempo' tiene 7 columnas con tipos como TEXT (cadenas), INTEGER (enteros) y REAL (flotantes), esenciales para el Modelo ER.\n",
      "\n",
      "Tabla 'Tipo_Incidente':\n",
      "       name  type\n",
      "0     clase  TEXT\n",
      "1  gravedad  TEXT\n",
      "Interpretación: La tabla 'Tipo_Incidente' tiene 2 columnas con tipos como TEXT (cadenas), INTEGER (enteros) y REAL (flotantes), esenciales para el Modelo ER.\n",
      "\n",
      "Tabla 'Incidente':\n",
      "          name     type\n",
      "0     radicado     TEXT\n",
      "1        clase     TEXT\n",
      "2     gravedad     TEXT\n",
      "3       diseno     TEXT\n",
      "4  tipo_geocod     TEXT\n",
      "5     OBJECTID  INTEGER\n",
      "6         cbml     TEXT\n",
      "7        fecha     TEXT\n",
      "8         hora     TEXT\n",
      "Interpretación: La tabla 'Incidente' tiene 9 columnas con tipos como TEXT (cadenas), INTEGER (enteros) y REAL (flotantes), esenciales para el Modelo ER.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Nombres y tipos de columnas\n",
    "print(\"=== Nombres y Tipos de Columnas ===\")\n",
    "for tabla in tablas:\n",
    "    query = f\"PRAGMA table_info({tabla});\"\n",
    "    result = pd.read_sql_query(query, conn)\n",
    "    print(f\"Tabla '{tabla}':\")\n",
    "    print(result[['name', 'type']])\n",
    "    print(f\"Interpretación: La tabla '{tabla}' tiene {len(result)} columnas con tipos como TEXT (cadenas), INTEGER (enteros) y REAL (flotantes), esenciales para el Modelo ER.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a11d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Consulta con Filtro ===\n",
      "Resultado de SELECT * FROM Incidente WHERE clase = 'Choque' LIMIT 5:\n",
      "  radicado   clase    gravedad        diseno  \\\n",
      "0  1580885  Choque      HERIDO  Tramo de via   \n",
      "1  1585081  Choque  SOLO DAÑOS  Tramo de via   \n",
      "2  1581508  Choque  SOLO DAÑOS  Tramo de via   \n",
      "3  1578199  Choque      HERIDO  Tramo de via   \n",
      "4  1591061  Choque  SOLO DAÑOS  Tramo de via   \n",
      "\n",
      "                         tipo_geocod  OBJECTID  cbml                fecha  \\\n",
      "0                         Malla vial         1  1602  2017-05-05 00:00:00   \n",
      "1  Malla vial aproximada: CL S 4-43A         2  1421  2017-06-06 00:00:00   \n",
      "2                         Malla vial         3  1309  2017-05-10 00:00:00   \n",
      "3                         Malla vial         5  0715  2017-04-14 00:00:00   \n",
      "4                         Malla vial         6  0706  2017-07-25 00:00:00   \n",
      "\n",
      "       hora  \n",
      "0  02:00 PM  \n",
      "1  11:20 AM  \n",
      "2  12:00 PM  \n",
      "3  04:30 AM  \n",
      "4  10:20 AM  \n",
      "Interpretación: Esta consulta muestra hasta 5 incidentes de tipo 'Choque'. Si no hay resultados, no existen coincidencias; útil para análisis de patrones específicos.\n",
      "\n",
      "--- Fin de Consultas ---\n"
     ]
    }
   ],
   "source": [
    "# Consulta con filtro\n",
    "print(\"=== Consulta con Filtro ===\")\n",
    "query_filtro = \"SELECT * FROM Incidente WHERE clase = 'Choque' LIMIT 5;\"\n",
    "result_filtro = pd.read_sql_query(query_filtro, conn)\n",
    "print(\"Resultado de SELECT * FROM Incidente WHERE clase = 'Choque' LIMIT 5:\")\n",
    "print(result_filtro)\n",
    "print(f\"Interpretación: Esta consulta muestra hasta 5 incidentes de tipo 'Choque'. Si no hay resultados, no existen coincidencias; útil para análisis de patrones específicos.\\n\")\n",
    "# Cerrar conexión\n",
    "conn.close()\n",
    "print(\"--- Fin de Consultas ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
